{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fBpV8V0SL_gR",
    "outputId": "4bfbeed0-d58d-451d-f063-225c252cdb72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: corner in /usr/local/lib/python3.7/dist-packages (2.2.1)\n",
      "Requirement already satisfied: matplotlib>=2.1 in /usr/local/lib/python3.7/dist-packages (from corner) (3.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (1.19.5)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.1->corner) (1.3.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.1->corner) (1.15.0)\n",
      "Requirement already satisfied: emcee in /usr/local/lib/python3.7/dist-packages (3.1.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from emcee) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install corner\n",
    "!pip install -U emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3DJ3EHsvQ8U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import emcee\n",
    "import corner\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LnNS_b7s2LI0"
   },
   "outputs": [],
   "source": [
    "GRB = pd.read_csv(os.path.join(dataDir,'GRB','GRB_data_Wang_et_al_2011.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "hikB4M77RoPS",
    "outputId": "8966dd3a-23e6-45b4-fd87-cea9a2174ecc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRB</th>\n",
       "      <th>z</th>\n",
       "      <th>P_bolo</th>\n",
       "      <th>P_bolo_err</th>\n",
       "      <th>S_bolo</th>\n",
       "      <th>S_bolo_err</th>\n",
       "      <th>F_beam</th>\n",
       "      <th>F_beam_err</th>\n",
       "      <th>T_lag</th>\n",
       "      <th>T_lag_err</th>\n",
       "      <th>V</th>\n",
       "      <th>V_err</th>\n",
       "      <th>E_peak</th>\n",
       "      <th>E_peak_err_min</th>\n",
       "      <th>E_peak_err_max</th>\n",
       "      <th>T_RT</th>\n",
       "      <th>T_RT_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>970228</td>\n",
       "      <td>0.70</td>\n",
       "      <td>7.300000e-06</td>\n",
       "      <td>4.300000e-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.010</td>\n",
       "      <td>115.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>970508</td>\n",
       "      <td>0.84</td>\n",
       "      <td>3.300000e-06</td>\n",
       "      <td>3.300000e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8.100000e-07</td>\n",
       "      <td>0.07950</td>\n",
       "      <td>0.02040</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>389.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>970828</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>1.100000e-06</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>1.200000e-05</td>\n",
       "      <td>0.00532</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.005</td>\n",
       "      <td>298.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>971214</td>\n",
       "      <td>3.42</td>\n",
       "      <td>7.500000e-07</td>\n",
       "      <td>2.400000e-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.002</td>\n",
       "      <td>190.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>980703</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.200000e-06</td>\n",
       "      <td>3.600000e-08</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>2.900000e-06</td>\n",
       "      <td>0.01840</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.001</td>\n",
       "      <td>254.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      GRB     z        P_bolo  ...  E_peak_err_max  T_RT  T_RT_err\n",
       "0  970228  0.70  7.300000e-06  ...            38.0   NaN       NaN\n",
       "1  970508  0.84  3.300000e-06  ...            40.0  0.65      0.07\n",
       "2  970828  0.96  1.000000e-05  ...            30.0  0.36      0.14\n",
       "3  971214  3.42  7.500000e-07  ...            20.0   NaN       NaN\n",
       "4  980703  0.97  1.200000e-06  ...            25.0  3.00      0.19\n",
       "\n",
       "[5 rows x 17 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GRB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "585V_ZEuzfWr"
   },
   "outputs": [],
   "source": [
    "def calculate_dL(mu):\n",
    "    '''\n",
    "    dL is the luminosity distance of GRB, which can be obtained from the reconstructed distance moduli of Pantheon\n",
    "    '''\n",
    "    \n",
    "    return 10**((mu-25)/5) * 10**6 * 3.086e+18\n",
    "\n",
    "def calculate_dL_err(dL, mu_err):\n",
    "\n",
    "    return abs(dL) * abs(np.log(10) * mu_err / 5.)\n",
    "\n",
    "def calculate_L(d_L, P_bolo):\n",
    "\n",
    "    '''\n",
    "    Assuming that GRBs radiate isotropically, the isotropic equivalent\n",
    "    luminosity can be derived from bolometric peak flux P_bolo\n",
    "    '''\n",
    "\n",
    "    return 4*np.pi*((d_L)**2)*P_bolo\n",
    "\n",
    "def calculate_L_err(dL_err, dL, P_bolo_err, P_bolo, L):\n",
    "\n",
    "    return abs(L) * np.sqrt((2. * dL_err / dL)**2 + (P_bolo_err / P_bolo)**2)\n",
    "\n",
    "def calculate_E_iso(dL, S_bolo, z):\n",
    "    '''\n",
    "    The isotropic equivalent energy E_iso can be obtained from the \n",
    "    bolometric fluence S_bolo by\n",
    "    '''\n",
    "\n",
    "    return 4*np.pi*(dL)**2*S_bolo / (1+z)\n",
    "\n",
    "def calculate_E_iso_err(L, dL_err, dL, S_bolo_err, S_bolo, z):\n",
    "    \n",
    "    return abs(L) * np.sqrt((2. * dL_err / dL)**2 + (S_bolo_err / S_bolo)**2) / (1+z)\n",
    "\n",
    "def calculate_E_gamma(E_iso, F_beam):\n",
    "    '''\n",
    "    If GRBs radiate in two symmetric beams, then we can define the \n",
    "    collimation-corrected energy E_gamma as\n",
    "    '''\n",
    "\n",
    "    return E_iso * F_beam\n",
    "\n",
    "def calculate_E_gamma_err(E_iso, F_beam, E_gamma, E_iso_err, F_beam_err):\n",
    "\n",
    "    return abs(E_gamma) * np.sqrt((E_iso_err / E_iso)**2 + (F_beam_err / F_beam)**2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juGDb4igqrO8"
   },
   "outputs": [],
   "source": [
    "# prepare sequential data\n",
    "def strided_app(a, L, S ):  # Window len = L, Stride len/stepsize = S\n",
    "    nrows = ((a.size-L)//S)+1\n",
    "    n = a.strides[0]\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=(nrows,L), strides=(S*n,n))\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10000\n",
    "sequence_len = 4\n",
    "n_features = 1 \n",
    "# number of neurons in LSTM\n",
    "num_units = 100\n",
    "\n",
    "def model_uncertainity(dropout_p=0.2):\n",
    "\n",
    "    inputs = layers.Input((sequence_len, n_features))\n",
    "    x = inputs\n",
    "    \n",
    "    x = layers.LSTM(100, return_sequences=True) (x)\n",
    "    #x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(dropout_p) (x, training=True)\n",
    "\n",
    "    x = layers.LSTM(100, return_sequences=True) (x)\n",
    "    #x = layers.BatchNormalization() (x)\n",
    "    x = layers.Dropout(dropout_p) (x, training=True)\n",
    "\n",
    "    x = layers.Dense(1) (x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "model_uncertainity = model_uncertainity()\n",
    "\n",
    "model_uncertainity.load_weights(os.path.join(runDir,'cp_union2_1.hdf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4762BngWRqjC"
   },
   "outputs": [],
   "source": [
    "zGRB_reconstruct = np.expand_dims(strided_app(GRB['z'].to_numpy(), 4, 4), axis=-1)\n",
    "\n",
    "mu_reconstruct_uncertainity = []\n",
    "n = 1000\n",
    "for i in range(n):\n",
    "    mu_reconstruct_uncertainity.append(model_uncertainity.predict(zGRB_reconstruct).flatten())\n",
    "\n",
    "mu_reconstruct_uncertainity = np.array(mu_reconstruct_uncertainity)\n",
    "\n",
    "GRB['mu'] = np.mean(mu_reconstruct_uncertainity, axis=0)\n",
    "GRB['mu_err'] = np.std(mu_reconstruct_uncertainity, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "256hUnGu565i"
   },
   "outputs": [],
   "source": [
    "# get distance moduli for redshifts using reconstructed mu-z funciton\n",
    "GRB['dL'] = calculate_dL(GRB['mu'].to_numpy()) \n",
    "GRB['dL_err'] = calculate_dL_err(GRB['dL'].to_numpy(), GRB['mu_err'].to_numpy())\n",
    "GRB['L'] = calculate_L(GRB['dL'].to_numpy(), GRB['P_bolo'].to_numpy()) \n",
    "GRB['L_err'] = calculate_L_err(GRB['dL_err'].to_numpy(), GRB['dL'].to_numpy(), GRB['P_bolo_err'].to_numpy(), GRB['P_bolo'].to_numpy(), GRB['L'].to_numpy()) \n",
    "GRB['E_iso'] = calculate_E_iso(GRB['dL'].to_numpy(), GRB['S_bolo'].to_numpy(), GRB['z'].to_numpy())\n",
    "GRB['E_iso_err'] = calculate_E_iso_err(GRB['L'].to_numpy(), GRB['dL_err'].to_numpy(), GRB['dL'].to_numpy(), GRB['S_bolo_err'].to_numpy(), GRB['S_bolo'].to_numpy(), GRB['z'].to_numpy())\n",
    "GRB['E_gamma'] = calculate_E_gamma(GRB['E_iso'].to_numpy(), GRB['F_beam'].to_numpy())\n",
    "GRB['E_gamma_err'] = calculate_E_gamma_err(GRB['E_iso'].to_numpy(), GRB['F_beam'].to_numpy(), GRB['E_gamma'].to_numpy(), GRB['E_iso_err'].to_numpy(), GRB['F_beam_err'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzXnFX391x6N"
   },
   "outputs": [],
   "source": [
    "# Since E_peak, error bars are asymmetric, just take the average of the lower and upper errors and then do the fit.\n",
    "GRB['E_peak_err'] = GRB[['E_peak_err_min','E_peak_err_max']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jCV9eJdK3oQI"
   },
   "source": [
    " quantities with a subscript ‘i’ indicate that they are measured in the comoving frame, which are related to the\n",
    "quantities in the observer frame by τlag,i = τlag(1 + z)\n",
    "−1\n",
    ", τRT,i = τRT(1 + z)\n",
    "−1\n",
    ", Vi = V (1 + z) and Ep,i = Ep(1 + z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eRzQl8Bc3rUT"
   },
   "outputs": [],
   "source": [
    "GRB['T_lag_i'] = GRB['T_lag'] / (1. + GRB['z'])\n",
    "GRB['T_lag_i_err'] = GRB['T_lag_err'] / (1. + GRB['z'])\n",
    "GRB['T_RT_i'] = GRB['T_RT'] / (1. + GRB['z'])\n",
    "GRB['T_RT_i_err'] = GRB['T_RT_err'] / (1. + GRB['z'])\n",
    "GRB['V_i'] = GRB['V'] * (1. + GRB['z'])\n",
    "GRB['V_i_err'] = GRB['V_err'] * (1. + GRB['z'])\n",
    "GRB['E_peak_i'] = GRB['E_peak'] * (1. + GRB['z'])\n",
    "GRB['E_peak_i_err'] = GRB['E_peak_err'] * (1. + GRB['z'])\n",
    "GRB['E_peak_i_err_min'] = GRB['E_peak_err_min'] * (1. + GRB['z'])\n",
    "GRB['E_peak_i_err_max'] = GRB['E_peak_err_max'] * (1. + GRB['z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDERA7baoWH6"
   },
   "outputs": [],
   "source": [
    "# normalized\n",
    "GRB['_T_lag_i'] = GRB['T_lag_i'] / 0.1\n",
    "GRB['_T_lag_i_err'] = GRB['T_lag_i_err'] / 0.1\n",
    "GRB['_V_i'] = GRB['V_i'] / 0.02\n",
    "GRB['_V_i_err'] = GRB['V_i_err'] / 0.02\n",
    "GRB['_E_peak_i'] = GRB['E_peak_i'] / 300\n",
    "GRB['_E_peak_i_err'] = GRB['E_peak_i_err'] / 300\n",
    "GRB['_E_peak_i_err_min'] = GRB['E_peak_i_err_min'] / 300\n",
    "GRB['_E_peak_i_err_max'] = GRB['E_peak_i_err_max'] / 300\n",
    "GRB['_T_RT_i'] = GRB['T_RT_i'] / 0.1\n",
    "GRB['_T_RT_i_err'] = GRB['T_RT_i_err'] / 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M3qCswSbz-Kb",
    "outputId": "04c00026-f4d7-455f-9ae4-76863dfe5a49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/pandas/core/series.py:726: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# log transformation\n",
    "GRB['log_T_lag_i'] = np.log10(GRB['_T_lag_i'])\n",
    "GRB['log_T_lag_i_err'] = abs(GRB['_T_lag_i_err'] / (GRB['_T_lag_i'] * np.log(10)))\n",
    "GRB['log_V_i'] = np.log10(GRB['_V_i'])\n",
    "GRB['log_V_i_err'] = abs(GRB['_V_i_err'] / (GRB['_V_i'] * np.log(10)))\n",
    "GRB['log_E_peak_i'] = np.log10(GRB['_E_peak_i'])\n",
    "GRB['log_E_peak_i_err'] = abs(GRB['_E_peak_i_err'] / (GRB['_E_peak_i'] * np.log(10)))\n",
    "GRB['log_E_peak_i_err_min'] = abs(GRB['_E_peak_i_err_min'] / (GRB['_E_peak_i'] * np.log(10)))\n",
    "GRB['log_E_peak_i_err_max'] = abs(GRB['_E_peak_i_err_max'] / (GRB['_E_peak_i'] * np.log(10)))\n",
    "GRB['log_T_RT_i'] = np.log10(GRB['_T_RT_i'])\n",
    "GRB['log_T_RT_i_err'] = abs(GRB['_T_RT_i_err'] / (GRB['_T_RT_i'] * np.log(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUSinXmz2CZM"
   },
   "outputs": [],
   "source": [
    "GRB['log_L'] = np.log10(GRB['L'])\n",
    "GRB['log_L_err'] = abs(GRB['L_err'] / (GRB['L'] * np.log(10)))\n",
    "GRB['log_E_iso'] = np.log10(GRB['E_iso'])\n",
    "GRB['log_E_iso_err'] = abs(GRB['E_iso_err'] / (GRB['E_iso'] * np.log(10)))\n",
    "GRB['log_E_gamma'] = np.log10(GRB['E_gamma'])\n",
    "GRB['log_E_gamma_err'] = abs(GRB['E_gamma_err'] / (GRB['E_gamma'] * np.log(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KffBF3g55R4"
   },
   "outputs": [],
   "source": [
    "# split the data in to low-z, high-z and All-z\n",
    "low_z_GRB = GRB[GRB['z']<=1.4]\n",
    "high_z_GRB = GRB[GRB['z']>1.4]\n",
    "All_z_GRB = GRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpCO1hgtybNj"
   },
   "outputs": [],
   "source": [
    "# filter GRBs seperately for each correlation, such that required data are present for each GRB\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sVONzeqtH5vK"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, xerr, yerr):\n",
    "    b, a, sigma_int = theta\n",
    "    model = b * x + a\n",
    "    sigma2 = sigma_int**2 + yerr**2 + b**2 * xerr**2\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "def log_prior(theta):\n",
    "    b, a, sigma_int = theta\n",
    "    if sigma_int > 0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_posterior(theta, x, y, xerr, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, xerr, yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 884
    },
    "id": "-yPAqW7e7pDi",
    "outputId": "7324f2d0-87c1-4528-e775-e50394fcf834",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlations = {\n",
    "    'T_lag-L' : ('log_T_lag_i', 'log_L', 'log_T_lag_i_err', 'log_L_err'),\n",
    "    'V-L' : ('log_V_i', 'log_E_iso', 'log_V_i_err', 'log_E_iso_err'),\n",
    "    'E_peak-L' : ('log_E_peak_i', 'log_L', 'log_E_peak_i_err', 'log_L_err'),\n",
    "    'E_peak-E_gamma' : ('log_E_peak_i', 'log_E_gamma', 'log_E_peak_i_err', 'log_E_gamma_err'),\n",
    "    'T_RT-L' : ('log_T_RT_i', 'log_L', 'log_T_RT_i_err', 'log_L_err'),\n",
    "    'E_peak-E_iso' : ('log_E_peak_i', 'log_E_iso', 'log_E_peak_i_err', 'log_E_iso_err')\n",
    "}\n",
    "\n",
    "# Calculate best fit line with uncertainities(Bayesian approach) using emcee\n",
    "GRB_samples = (low_z_GRB, high_z_GRB, All_z_GRB)\n",
    "sample_types =  ('low-z', 'high-z', 'All-z')\n",
    "colors = ('b','r','k')\n",
    "\n",
    "# MCMC parameters\n",
    "nwalkers, ndim = 64, 3\n",
    "nsteps, nburns = 10000, 5000\n",
    "\n",
    "# empty dictionary to save best fit parameters and uncertainities\n",
    "BestFitParameters = { \n",
    "    'T_lag-L':{'low-z':None, 'high-z':None, 'All-z':None},\n",
    "    'V-L':{'low-z':None, 'high-z':None, 'All-z':None},\n",
    "    'E_peak-L':{'low-z':None, 'high-z':None, 'All-z':None},\n",
    "    'E_peak-E_gamma':{'low-z':None, 'high-z':None, 'All-z':None},\n",
    "    'T_RT-L':{'low-z':None, 'high-z':None, 'All-z':None},\n",
    "    'E_peak-E_iso':{'low-z':None, 'high-z':None, 'All-z':None}\n",
    "}\n",
    "\n",
    "for i, correlation in enumerate(correlations):\n",
    "\n",
    "    luminosities = correlations[correlation]\n",
    "\n",
    "    '''\n",
    "    # create empty figure object for time series (steps) plot of parameters in chain\n",
    "    fig1 = plt.figure(constrained_layout=True, figsize=(15,5))\n",
    "    gs1 = gridspec.GridSpec(nrows=1, ncols=3, figure=fig1)\n",
    "    '''\n",
    "\n",
    "    # create empty figure object for corner plots (confidence contours and marginalized PDFs of parameters)\n",
    "    fig2 = plt.figure(figsize=(5, 5))\n",
    "    fig2.patch.set_facecolor('white')\n",
    "\n",
    "    for k, (GRB_sample, sample_type, color) in enumerate(zip(GRB_samples, sample_types, colors)):\n",
    "\n",
    "        df = GRB_sample.filter(luminosities).dropna()\n",
    "        \n",
    "        x = df[luminosities[0]].to_numpy()\n",
    "        y = df[luminosities[1]].to_numpy()\n",
    "        xerr = df[luminosities[2]].to_numpy()\n",
    "        yerr = df[luminosities[3]].to_numpy()\n",
    "\n",
    "        starting_guesses = np.random.rand(nwalkers, ndim)\n",
    "        \n",
    "        sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x, y, xerr, yerr))\n",
    "        sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "        labels = ['b', 'a', 'sigma_int']\n",
    "        '''\n",
    "        # time series plot of parameters\n",
    "        samples = sampler.get_chain()\n",
    "        gs11 = gridspec.GridSpecFromSubplotSpec(nrows=3, ncols=1, subplot_spec=gs1[k])\n",
    "\n",
    "        for j in range(ndim):\n",
    "            ax = fig1.add_subplot(gs11[j])\n",
    "            ax.plot(samples[..., j], 'k', alpha=0.3)\n",
    "            ax.set_xlim(0, len(samples))\n",
    "            ax.set_ylabel(labels[j])\n",
    "            ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "        #axes[-1].set_xlabel(\"step number\");\n",
    "        '''\n",
    "        # corner plots\n",
    "        flat_samples = sampler.get_chain(discard=nburns, flat=True)\n",
    "        corner.corner(flat_samples, labels=labels, color=color,fig=fig2)\n",
    "\n",
    "        # save best fit values(mean) and uncertainities(std) of parameters in a dictionary\n",
    "        BestFitParameters[correlation][sample_type] = {\n",
    "                'a' : np.mean(flat_samples[:, 1]), \n",
    "                'a_err' : np.std(flat_samples[:, 1]),\n",
    "                'b' : np.mean(flat_samples[:, 0]), \n",
    "                'b_err' : np.std(flat_samples[:, 0]),\n",
    "                'sigma_int' : np.mean(flat_samples[:, 2]),\n",
    "                'sigma_int_err' : np.std(flat_samples[:, 2])\n",
    "        }\n",
    "    \n",
    "        fig2.axes[0].annotate(sample_type, xy=(0.95*2.5, 0.95-k*0.2), xycoords='axes fraction',color=color)\n",
    "  \n",
    "    fig2.suptitle(correlation)\n",
    "\n",
    "    #fig1.savefig('time_series_of_params.png')\n",
    "    fig2.savefig(correlation+'_corner_plot.png')\n",
    "    #fig1.show()\n",
    "    fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwibAn2fm5bV"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(runDir, 'BestFitParameters.pickle'), 'wb') as handle:\n",
    "    pickle.dump(BestFitParameters, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-b8XSKRAoKPg"
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(runDir, 'BestFitParameters.pickle'), 'rb') as handle:\n",
    "    BestFitParameters = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJyGb68Prm0s"
   },
   "outputs": [],
   "source": [
    "def flatten_dict(nested_dict):\n",
    "    res = {}\n",
    "    if isinstance(nested_dict, dict):\n",
    "        for k in nested_dict:\n",
    "            flattened_dict = flatten_dict(nested_dict[k])\n",
    "            for key, val in flattened_dict.items():\n",
    "                key = list(key)\n",
    "                key.insert(0, k)\n",
    "                res[tuple(key)] = val\n",
    "    else:\n",
    "        res[()] = nested_dict\n",
    "    return res\n",
    "\n",
    "\n",
    "def nested_dict_to_df(values_dict):\n",
    "    flat_dict = flatten_dict(values_dict)\n",
    "    df = pd.DataFrame.from_dict(flat_dict, orient=\"index\")\n",
    "    df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "    df = df.unstack(level=-1)\n",
    "    df.columns = df.columns.map(\"{0[1]}\".format)\n",
    "    return df\n",
    "\n",
    "table = nested_dict_to_df(BestFitParameters)\n",
    "\n",
    "!pip install tabulate\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(tabulate(table, headers='keys', tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBg8IP8Ry9-k"
   },
   "outputs": [],
   "source": [
    "correlations = {\n",
    "    'T_lag-L' : {'features': ('log_T_lag_i', 'log_L', 'log_T_lag_i_err', 'log_L_err'), 'ylim':(50, 54), 'xlim':(-2.0,2.0)},\n",
    "    'V-L' : {'features': ('log_V_i', 'log_L', 'log_V_i_err', 'log_L_err'), 'ylim':(50, 54), 'xlim':(-1.0,2.0)},\n",
    "    'E_peak-L' : {'features': ('log_E_peak_i', 'log_L', 'log_E_peak_i_err', 'log_L_err'), 'ylim':(49, 54), 'xlim':(-1.5,1.5)},\n",
    "    'E_peak-E_gamma' : {'features': ('log_E_peak_i', 'log_E_gamma', 'log_E_peak_i_err', 'log_E_gamma_err'), 'ylim':(48, 52), 'xlim':(-1.5,1.5)},\n",
    "    'T_RT-L' : {'features': ('log_T_RT_i', 'log_L', 'log_T_RT_i_err', 'log_L_err'), 'ylim':(50, 54), 'xlim':(-1.0,2.0)},\n",
    "    'E_peak-E_iso' : {'features': ('log_E_peak_i', 'log_E_iso', 'log_E_peak_i_err', 'log_E_iso_err'), 'ylim':(50, 55), 'xlim':(-1.5,1.5)}\n",
    "}\n",
    "\n",
    "GRB_samples = (low_z_GRB, high_z_GRB, All_z_GRB)\n",
    "sample_types =  ('low-z', 'high-z', 'All-z')\n",
    "colors = ('b','r','k')\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15,20))\n",
    "fig.patch.set_facecolor('white')\n",
    "# plot best fit for each luminosity correlation\n",
    "for i, correlation in enumerate(correlations):\n",
    "\n",
    "    luminosities = correlations[correlation]['features']\n",
    "\n",
    "    # create empty figures with no axis\n",
    "    #plt.figure(figsize=(15,5))\n",
    "\n",
    "    #plt.subplot(121)\n",
    "\n",
    "    for GRB_sample, sample_type, color in zip(GRB_samples, sample_types, colors):\n",
    "        \n",
    "        df = GRB_sample.filter(luminosities).dropna()\n",
    "            \n",
    "        x = df[luminosities[0]].to_numpy()\n",
    "        y = df[luminosities[1]].to_numpy()\n",
    "        xerr = df[luminosities[2]].to_numpy()\n",
    "        yerr = df[luminosities[3]].to_numpy()\n",
    "\n",
    "        if sample_type != 'All-z':\n",
    "            axs[int(i/2), i%2].errorbar(x,y,xerr=xerr,yerr=yerr,fmt='.', ecolor=color, label=sample_type)\n",
    "        \n",
    "        axs[int(i/2), i%2].plot(\n",
    "            np.linspace(-2,2,100), \n",
    "            BestFitParameters[correlation][sample_type]['a'] + BestFitParameters[correlation][sample_type]['b'] * np.linspace(-2,2,100),\n",
    "            #luminosity_correlation_fit(np.linspace(-2,2,100), 'T_lag-L', sample_type), \n",
    "            linestyle='-', color=color, label=sample_type +' best fit'\n",
    "            )\n",
    "\n",
    "    axs[int(i/2), i%2].set_title(correlation)\n",
    "    axs[int(i/2), i%2].set_xlabel(luminosities[0])\n",
    "    axs[int(i/2), i%2].set_ylabel(luminosities[1])\n",
    "    axs[int(i/2), i%2].set_ylim(correlations[correlation]['ylim'])\n",
    "    axs[int(i/2), i%2].set_xlim(correlations[correlation]['xlim'])\n",
    "        \n",
    "    axs[int(i/2), i%2].legend()\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBPxbyPg8JzE"
   },
   "outputs": [],
   "source": [
    "# E_peak -> E_gamma -> E_iso -> d_L\n",
    "\n",
    "def caliberate_d_L(log_E_peak_i, log_E_peak_i_err, F_beam, F_beam_err, S_bolo, S_bolo_err, z):\n",
    "\n",
    "    def calculate_E_gamma_from_correlation(log_E_peak_i, log_E_peak_i_err):\n",
    "\n",
    "        # get best fit parameters for correlation\n",
    "        a = BestFitParameters['E_peak-E_gamma']['All-z']['a']\n",
    "        a_err = BestFitParameters['E_peak-E_gamma']['All-z']['a_err']\n",
    "        b = BestFitParameters['E_peak-E_gamma']['All-z']['b']\n",
    "        b_err = BestFitParameters['E_peak-E_gamma']['All-z']['b_err']\n",
    "\n",
    "        # log transformed, normalized E_peak\n",
    "        log_E_gamma = a + b * log_E_peak_i\n",
    "        log_E_gamma_err = np.sqrt(a_err**2 + (abs(b * log_E_peak_i) * np.sqrt((b_err/b)**2 + (log_E_peak_i_err/log_E_peak_i)**2))**2)\n",
    "\n",
    "        E_gamma = 10 ** log_E_gamma\n",
    "\n",
    "        E_gamma_err = abs(E_gamma) * abs(np.log(10) * log_E_gamma_err)\n",
    "\n",
    "        return E_gamma, E_gamma_err\n",
    "\n",
    "    def calculate_E_iso(E_gamma, E_gamma_err, F_beam, F_beam_err):\n",
    "\n",
    "        E_iso = E_gamma / F_beam\n",
    "        E_iso_err = abs(E_iso) * np.sqrt( (E_gamma_err/E_gamma)**2 + (F_beam_err/F_beam)**2 )\n",
    "\n",
    "        return E_iso, E_iso_err\n",
    "    \n",
    "\n",
    "    def calculate_d_L(E_iso, E_iso_err, S_bolo, S_bolo_err, z):\n",
    "\n",
    "        # calculate d_L\n",
    "        a = E_iso * (1. + z)\n",
    "        a_err = (1.+z) * E_iso_err\n",
    "\n",
    "        b = 4. * np.pi * S_bolo\n",
    "        b_err = 4. * np.pi * S_bolo_err\n",
    "\n",
    "        c = a/b\n",
    "        c_err = abs(c) * np.sqrt((a_err/a)**2 + (b_err/b)**2)\n",
    "\n",
    "        d_L = np.sqrt(c) \n",
    "        d_L_err = c_err / (2*np.sqrt(c))\n",
    "    \n",
    "        return d_L, d_L_err\n",
    "    \n",
    "    # calculate E_gamma from E_gamma-E_peak correlation\n",
    "    E_gamma, E_gamma_err = calculate_E_gamma_from_correlation(log_E_peak_i, log_E_peak_i_err)\n",
    "\n",
    "    # calculate E_iso\n",
    "    E_iso, E_iso_err = calculate_E_iso(E_gamma, E_gamma_err, F_beam, F_beam_err)\n",
    "\n",
    "    # calculate d_L\n",
    "    d_L, d_L_err = calculate_d_L(E_iso, E_iso_err, S_bolo, S_bolo_err, z)\n",
    "\n",
    "    return d_L, d_L_err\n",
    "\n",
    "def caliberate_mu(d_L, d_L_err):\n",
    "\n",
    "    # calculate mu\n",
    "    mu = 5. * np.log10(d_L / (10**6 * 3.086e+18)) + 25.\n",
    "    mu_err = abs(5. * d_L_err / (d_L * np.log(10))) \n",
    "\n",
    "    return mu, mu_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2eZiBWLiJW_"
   },
   "outputs": [],
   "source": [
    "# caliberate d_L from E_peak - E_gamma relation, hence only those GRBs with \n",
    "# sufficient data(E_peak, F_beam, S_bolo) can be caliberated using from this \n",
    "filtered_GRB = GRB[['E_gamma', 'E_gamma_err','log_E_peak_i', 'log_E_peak_i_err', 'F_beam', 'F_beam_err', 'S_bolo', 'S_bolo_err', 'z']].dropna()\n",
    "\n",
    "d_L, d_L_err = caliberate_d_L(\n",
    "    filtered_GRB['log_E_peak_i'].to_numpy(), \n",
    "    filtered_GRB['log_E_peak_i_err'].to_numpy(),\n",
    "    filtered_GRB['F_beam'].to_numpy(),\n",
    "    filtered_GRB['F_beam_err'].to_numpy(),\n",
    "    filtered_GRB['S_bolo'].to_numpy(),\n",
    "    filtered_GRB['S_bolo_err'].to_numpy(),\n",
    "    filtered_GRB['z'].to_numpy())\n",
    "\n",
    "mu_GRB, mu_err_GRB = caliberate_mu(d_L, d_L_err)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "ax.errorbar(filtered_GRB['z'].to_numpy(), mu_GRB, yerr=mu_err_GRB, fmt='.', color='b', capsize=2, label='GRB')\n",
    "ax.errorbar(data1['zCMB'], data1['MU'], yerr=data1['MUERR'], fmt='.r', capsize=2, label='Pantheon sample')\n",
    "ax.legend()\n",
    "ax.set_xlabel('z')\n",
    "ax.set_ylabel('mu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rx4d95H2aLQ"
   },
   "outputs": [],
   "source": [
    "# estimate omega_M and omega_L\n",
    "\n",
    "# calculate H(z) for all z\n",
    "\n",
    "# speed of light\n",
    "c = 9.7 * 10e-15 # Mpc/sec\n",
    "z = filtered_GRB['z'].to_numpy()\n",
    "H_z = c * (1. + z)**2 * np.log(1.+z) / d_L\n",
    "H_z_err = abs(H_z) * d_L_err / d_L\n",
    "H_0 = 67.4 # km/sec/Mpc\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkGaFbkIGtXk"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, xerr, yerr):\n",
    "    b, a, sigma_int = theta\n",
    "    model = b * x + a\n",
    "    sigma2 = sigma_int**2 + yerr**2 + b**2 * xerr**2\n",
    "    return -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))\n",
    "\n",
    "def log_prior(theta):\n",
    "    b, a, sigma_int = theta\n",
    "    if sigma_int > 0:\n",
    "        return 0.0\n",
    "    return -np.inf\n",
    "\n",
    "def log_posterior(theta, x, y, xerr, yerr):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y, xerr, yerr)\n",
    "\n",
    "# MCMC parameters\n",
    "nwalkers, ndim = 64, 3\n",
    "nsteps, nburns = 10000, 5000\n",
    "\n",
    "\n",
    "# create empty figure object for corner plots (confidence contours and marginalized PDFs of parameters)\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "fig2.patch.set_facecolor('white')\n",
    "\n",
    "x = (1. + z)**3\n",
    "y = (1. + z)**2 #(H_z / H_0)**2\n",
    "\n",
    "xerr = 0\n",
    "yerr = 0 #abs(y * 2 * H_z_err / H_z)# should errors in H_0 be considered ?\n",
    "\n",
    "starting_guesses = np.random.rand(nwalkers, ndim)\n",
    "        \n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x, y, xerr, yerr))\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "labels = ['omega_L', 'omega_M','sigma_int']\n",
    "'''\n",
    "# time series plot of parameters\n",
    "samples = sampler.get_chain()\n",
    "gs11 = gridspec.GridSpecFromSubplotSpec(nrows=3, ncols=1, subplot_spec=gs1[k])\n",
    "\n",
    "for j in range(ndim):\n",
    "    ax = fig1.add_subplot(gs11[j])\n",
    "    ax.plot(samples[..., j], 'k', alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[j])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "    #axes[-1].set_xlabel(\"step number\");\n",
    "'''\n",
    "# corner plots\n",
    "flat_samples = sampler.get_chain(discard=nburns, flat=True)\n",
    "corner.corner(flat_samples[:,:2], labels=labels[:2], color=color,fig=fig2)\n",
    "\n",
    "# save best fit values(mean) and uncertainities(std) of parameters in a dictionary\n",
    "omega_M_omega_L_correlation_best_fit = {\n",
    "        'omega_L' : np.mean(flat_samples[:, 1]), \n",
    "        'omega_L_err' : np.std(flat_samples[:, 1]),\n",
    "        'omega_M' : np.mean(flat_samples[:, 0]), \n",
    "        'omega_M_err' : np.std(flat_samples[:, 0]),\n",
    "        #'sigma_int' : np.mean(flat_samples[:, 2]),\n",
    "        #'sigma_int_err' : np.std(flat_samples[:, 2])\n",
    "    }\n",
    "\n",
    "#fig2.axes[0].annotate(sample_type, xy=(0.95*2.5, 0.95-k*0.2), xycoords='axes fraction',color=color)\n",
    "\n",
    "fig2.suptitle('omega_M- omega_L')\n",
    "\n",
    "#fig1.savefig('time_series_of_params.png')\n",
    "fig2.savefig('omega_M_omega_L_corner_plot.png')\n",
    "#fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YDwUsf2BXR07"
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y):\n",
    "    b = theta\n",
    "    model = b * x + 1\n",
    "    return -0.5 * np.sum((y - model) ** 2)\n",
    "\n",
    "def log_prior(theta):\n",
    "    b = theta\n",
    "    return 0.\n",
    "\n",
    "def log_posterior(theta, x, y):\n",
    "    lp = log_prior(theta)\n",
    "    if not np.isfinite(lp):\n",
    "        return -np.inf\n",
    "    return lp + log_likelihood(theta, x, y)\n",
    "\n",
    "# MCMC parameters\n",
    "nwalkers, ndim = 64, 1\n",
    "nsteps, nburns = 10000, 5000\n",
    "\n",
    "\n",
    "# create empty figure object for corner plots (confidence contours and marginalized PDFs of parameters)\n",
    "fig2 = plt.figure(figsize=(5, 5))\n",
    "fig2.patch.set_facecolor('white')\n",
    "\n",
    "x = (1. + z)**3 - 1.\n",
    "y = (1. + z)**2 #(H_z / H_0)**2\n",
    "\n",
    "xerr = 0\n",
    "yerr = 0 #abs(y * 2 * H_z_err / H_z)# should errors in H_0 be considered ?\n",
    "\n",
    "starting_guesses = np.random.rand(nwalkers, ndim)\n",
    "        \n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=(x, y))\n",
    "sampler.run_mcmc(starting_guesses, nsteps)\n",
    "\n",
    "labels = ['omega_L', 'omega_M','sigma_int']\n",
    "'''\n",
    "# time series plot of parameters\n",
    "samples = sampler.get_chain()\n",
    "gs11 = gridspec.GridSpecFromSubplotSpec(nrows=3, ncols=1, subplot_spec=gs1[k])\n",
    "\n",
    "for j in range(ndim):\n",
    "    ax = fig1.add_subplot(gs11[j])\n",
    "    ax.plot(samples[..., j], 'k', alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[j])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "    #axes[-1].set_xlabel(\"step number\");\n",
    "'''\n",
    "# corner plots\n",
    "flat_samples = sampler.get_chain(discard=nburns, flat=True)\n",
    "corner.corner(flat_samples, labels='omega_M', color=color,fig=fig2)\n",
    "\n",
    "# save best fit values(mean) and uncertainities(std) of parameters in a dictionary\n",
    "omega_M_omega_L_correlation_best_fit = {\n",
    "        'omega_L' : np.mean(flat_samples), \n",
    "        'omega_L_err' : np.std(flat_samples),\n",
    "        'omega_M' : np.mean(flat_samples), \n",
    "        'omega_M_err' : np.std(flat_samples),\n",
    "        #'sigma_int' : np.mean(flat_samples[:, 2]),\n",
    "        #'sigma_int_err' : np.std(flat_samples[:, 2])\n",
    "    }\n",
    "\n",
    "#fig2.axes[0].annotate(sample_type, xy=(0.95*2.5, 0.95-k*0.2), xycoords='axes fraction',color=color)\n",
    "\n",
    "fig2.suptitle('omega_M')\n",
    "\n",
    "#fig1.savefig('time_series_of_params.png')\n",
    "fig2.savefig('omega_M_corner_plot.png')\n",
    "#fig1.show()\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x7Hekw047Ha"
   },
   "outputs": [],
   "source": [
    "print('omega_M = {}'.format(omega_M_omega_L_correlation_best_fit['omega_M']))\n",
    "print('omega_M_err = {}'.format(omega_M_omega_L_correlation_best_fit['omega_M_err']))\n",
    "\n",
    "#print('omega_L = {}'.format(omega_M_omega_L_correlation_best_fit['omega_L']))\n",
    "#print('omega_L_err = {}'.format(omega_M_omega_L_correlation_best_fit['omega_L_err']))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01-.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
